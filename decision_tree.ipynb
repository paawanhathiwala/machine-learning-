{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10596423,"sourceType":"datasetVersion","datasetId":6558741}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.489099Z","iopub.execute_input":"2025-01-27T21:10:31.489469Z","iopub.status.idle":"2025-01-27T21:10:31.494327Z","shell.execute_reply.started":"2025-01-27T21:10:31.489440Z","shell.execute_reply":"2025-01-27T21:10:31.493157Z"}},"outputs":[],"execution_count":149},{"cell_type":"code","source":"dataSet = \"/kaggle/input/irdatadec/Iris (1).csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.495765Z","iopub.execute_input":"2025-01-27T21:10:31.496214Z","iopub.status.idle":"2025-01-27T21:10:31.517476Z","shell.execute_reply.started":"2025-01-27T21:10:31.496170Z","shell.execute_reply":"2025-01-27T21:10:31.516277Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"iris = pd.read_csv(dataSet)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.519715Z","iopub.execute_input":"2025-01-27T21:10:31.520177Z","iopub.status.idle":"2025-01-27T21:10:31.543977Z","shell.execute_reply.started":"2025-01-27T21:10:31.520136Z","shell.execute_reply":"2025-01-27T21:10:31.542529Z"}},"outputs":[],"execution_count":151},{"cell_type":"code","source":"iris['Species'] = iris['Species'].map({\n    'Iris-setosa': 0, \n    'Iris-versicolor': 1, \n    'Iris-virginica': 2\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.545696Z","iopub.execute_input":"2025-01-27T21:10:31.546177Z","iopub.status.idle":"2025-01-27T21:10:31.554434Z","shell.execute_reply.started":"2025-01-27T21:10:31.546137Z","shell.execute_reply":"2025-01-27T21:10:31.553198Z"}},"outputs":[],"execution_count":152},{"cell_type":"code","source":"x = iris[['SepalLengthCm', \"SepalWidthCm\" ,\t\"PetalLengthCm\" ,\"PetalWidthCm\" ]].values\ny = iris['Species'].values\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.555680Z","iopub.execute_input":"2025-01-27T21:10:31.556037Z","iopub.status.idle":"2025-01-27T21:10:31.576414Z","shell.execute_reply.started":"2025-01-27T21:10:31.555995Z","shell.execute_reply":"2025-01-27T21:10:31.575113Z"}},"outputs":[],"execution_count":153},{"cell_type":"code","source":"def entropy(y):\n    \"\"\"Calculate the entropy of a dataset\"\"\"\n    unique_classes, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    return -np.sum(probabilities * np.log2(probabilities))\n\ndef information_gain(X_column, y, threshold):\n    \"\"\"\n    Calculate information gain for a split on a feature\n    X_column: The feature column to split on\n    y: The target column\n    threshold: The threshold value for splitting\n    \"\"\"\n    left_indices = X_column <= threshold\n    right_indices = X_column > threshold\n\n    # Calculate weighted average entropy after the split\n    n = len(y)\n    left_entropy = entropy(y[left_indices])\n    right_entropy = entropy(y[right_indices])\n    weighted_avg_entropy = (len(y[left_indices]) / n) * left_entropy + (len(y[right_indices]) / n) * right_entropy\n\n    # Calculate information gain\n    return entropy(y) - weighted_avg_entropy\n\ndef find_best_split(X, y):\n    \"\"\"\n    Find the best feature and threshold to split on\n    X: Features dataset\n    y: Target column\n    \"\"\"\n    best_feature = None\n    best_threshold = None\n    best_info_gain = -1\n    for feature in range(X.shape[1]):  # Loop over each feature\n        X_column = X[:, feature]\n        thresholds = np.unique(X_column)\n\n        for threshold in thresholds:\n            gain = information_gain(X_column, y, threshold)\n            if gain > best_info_gain:\n                best_feature = feature\n                best_threshold = threshold\n                best_info_gain = gain\n\n    return best_feature, best_threshold\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.577512Z","iopub.execute_input":"2025-01-27T21:10:31.577909Z","iopub.status.idle":"2025-01-27T21:10:31.590636Z","shell.execute_reply.started":"2025-01-27T21:10:31.577869Z","shell.execute_reply":"2025-01-27T21:10:31.589540Z"}},"outputs":[],"execution_count":154},{"cell_type":"code","source":"class DecisionTree:\n    def __init__(self, max_depth=None):\n        self.max_depth = max_depth\n        self.tree = None\n\n    def fit(self, X, y, depth=0):\n        \"\"\"\n        Fit the training data to build the decision tree\n        X: Features dataset\n        y: Target column\n        depth: Current depth of the tree\n        \"\"\"\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n\n        # Stopping criteria\n        if n_classes == 1 or n_samples <= 1 or (self.max_depth is not None and depth >= self.max_depth):\n            most_common_class = np.argmax(np.bincount(y))\n            return most_common_class\n\n        # Find the best feature and threshold to split on\n        best_feature, best_threshold = find_best_split(X, y)\n\n        if best_feature is None:  # If no split is found, return the majority class\n            most_common_class = np.argmax(np.bincount(y))\n            return most_common_class\n\n        # Split the data into left and right subsets\n        left_indices = X[:, best_feature] <= best_threshold\n        right_indices = X[:, best_feature] > best_threshold\n\n        # Recursively build the tree\n        left_subtree = self.fit(X[left_indices], y[left_indices], depth + 1)\n        right_subtree = self.fit(X[right_indices], y[right_indices], depth + 1)\n\n        # Return a dictionary representation of the tree\n        self.tree = {\n            \"feature\": best_feature,\n            \"threshold\": best_threshold,\n            \"left\": left_subtree,\n            \"right\": right_subtree,\n        }\n        return self.tree\n\n    def predict_one(self, x, tree):\n        \"\"\"Predict a single sample\"\"\"\n        if not isinstance(tree, dict):\n            return tree\n\n        feature = tree[\"feature\"]\n        threshold = tree[\"threshold\"]\n\n        if x[feature] <= threshold:\n            return self.predict_one(x, tree[\"left\"])\n        else:\n            return self.predict_one(x, tree[\"right\"])\n\n    def predict(self, X):\n        \"\"\"Predict multiple samples\"\"\"\n        return [self.predict_one(x, self.tree) for x in X]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.591442Z","iopub.execute_input":"2025-01-27T21:10:31.591734Z","iopub.status.idle":"2025-01-27T21:10:31.609084Z","shell.execute_reply.started":"2025-01-27T21:10:31.591708Z","shell.execute_reply":"2025-01-27T21:10:31.607995Z"}},"outputs":[],"execution_count":155},{"cell_type":"code","source":"tree = DecisionTree(max_depth=3)\ntree.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.610612Z","iopub.execute_input":"2025-01-27T21:10:31.610891Z","iopub.status.idle":"2025-01-27T21:10:31.679026Z","shell.execute_reply.started":"2025-01-27T21:10:31.610869Z","shell.execute_reply":"2025-01-27T21:10:31.677621Z"}},"outputs":[{"execution_count":156,"output_type":"execute_result","data":{"text/plain":"{'feature': 2,\n 'threshold': 1.9,\n 'left': 0,\n 'right': {'feature': 2,\n  'threshold': 4.7,\n  'left': {'feature': 3, 'threshold': 1.5, 'left': 1, 'right': 2},\n  'right': {'feature': 2, 'threshold': 5.1, 'left': 2, 'right': 2}}}"},"metadata":{}}],"execution_count":156},{"cell_type":"code","source":"y_pred = tree.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.680747Z","iopub.execute_input":"2025-01-27T21:10:31.681160Z","iopub.status.idle":"2025-01-27T21:10:31.685620Z","shell.execute_reply.started":"2025-01-27T21:10:31.681120Z","shell.execute_reply":"2025-01-27T21:10:31.684571Z"}},"outputs":[],"execution_count":157},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Predictions: {y_pred}\")\nprint(f\"Actual Labels: {y_test}\")\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.687305Z","iopub.execute_input":"2025-01-27T21:10:31.687741Z","iopub.status.idle":"2025-01-27T21:10:31.709241Z","shell.execute_reply.started":"2025-01-27T21:10:31.687709Z","shell.execute_reply":"2025-01-27T21:10:31.708052Z"}},"outputs":[{"name":"stdout","text":"Predictions: [1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 2, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 2, 1, 0, 0]\nActual Labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n 0 0 0 2 1 1 0 0]\nAccuracy: 93.33%\n","output_type":"stream"}],"execution_count":158},{"cell_type":"code","source":"print(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.710651Z","iopub.execute_input":"2025-01-27T21:10:31.711033Z","iopub.status.idle":"2025-01-27T21:10:31.744005Z","shell.execute_reply.started":"2025-01-27T21:10:31.711003Z","shell.execute_reply":"2025-01-27T21:10:31.742486Z"}},"outputs":[{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        19\n           1       1.00      0.77      0.87        13\n           2       0.81      1.00      0.90        13\n\n    accuracy                           0.93        45\n   macro avg       0.94      0.92      0.92        45\nweighted avg       0.95      0.93      0.93        45\n\n","output_type":"stream"}],"execution_count":159},{"cell_type":"code","source":"print(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:31.745144Z","iopub.execute_input":"2025-01-27T21:10:31.745540Z","iopub.status.idle":"2025-01-27T21:10:31.753088Z","shell.execute_reply.started":"2025-01-27T21:10:31.745498Z","shell.execute_reply":"2025-01-27T21:10:31.751813Z"}},"outputs":[{"name":"stdout","text":"\nConfusion Matrix:\n[[19  0  0]\n [ 0 10  3]\n [ 0  0 13]]\n","output_type":"stream"}],"execution_count":160}]}